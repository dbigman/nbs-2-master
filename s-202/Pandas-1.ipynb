{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anonymous-academy",
   "metadata": {},
   "source": [
    "# Pandas Exploration I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-irrigation",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h3>Table of Contents<span class=\"tocSkip\"></span></h3>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Installation\" data-toc-modified-id=\"Installation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Installation</a></span></li><li><span><a href=\"#Introduction-to-pandas-data-structures\" data-toc-modified-id=\"Introduction-to-pandas-data-structures-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Introduction to pandas data structures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Series\" data-toc-modified-id=\"Series-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Series</a></span></li><li><span><a href=\"#Dataframes\" data-toc-modified-id=\"Dataframes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#From-data-types\" data-toc-modified-id=\"From-data-types-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>From data types</a></span></li><li><span><a href=\"#From-path\" data-toc-modified-id=\"From-path-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>From path</a></span></li><li><span><a href=\"#From-databases\" data-toc-modified-id=\"From-databases-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>From databases</a></span></li></ul></li></ul></li><li><span><a href=\"#Exploratory-analysis-of-a-dataframe\" data-toc-modified-id=\"Exploratory-analysis-of-a-dataframe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploratory analysis of a dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Meta-information\" data-toc-modified-id=\"Meta-information-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Meta information</a></span></li><li><span><a href=\"#Previsualization\" data-toc-modified-id=\"Previsualization-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Previsualization</a></span></li><li><span><a href=\"#Order-a-dataframe\" data-toc-modified-id=\"Order-a-dataframe-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Order a dataframe</a></span></li><li><span><a href=\"#NaN-values\" data-toc-modified-id=\"NaN-values-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>NaN values</a></span></li><li><span><a href=\"#Basic-descriptive-statistics\" data-toc-modified-id=\"Basic-descriptive-statistics-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Basic descriptive statistics</a></span></li></ul></li><li><span><a href=\"#Pandas-usual-methods\" data-toc-modified-id=\"Pandas-usual-methods-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Pandas usual methods</a></span></li><li><span><a href=\"#Further-materials\" data-toc-modified-id=\"Further-materials-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Further materials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-needle",
   "metadata": {},
   "source": [
    "![pandas](https://media.giphy.com/media/nVsLCrW5iHf6E/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fcd9d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751721df",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Pandas stands out as the preeminent library within the Python ecosystem for data manipulation and analysis. It boasts speed, power, flexibility, ease of use, and the open-source advantage.\n",
    "\n",
    "Pandas was conceived and developed by **Wes McKinney (A.K.A GOD MCKINNEY)**, a financial analyst turned software developer. Wes recognized the need for a tool that could effectively address the challenges of data analysis in the financial industry. His vision was to create a Python library that could provide the same data manipulation capabilities found in popular spreadsheet software and relational databases. \n",
    "\n",
    "In 2008, Wes McKinney began working on Pandas while at AQR Capital Management. His passion and dedication led to the release of the first version of Pandas in 2009. It didn't take long for Pandas to gain traction in the Python community, and it quickly became an essential tool for data analysts and scientists.\n",
    "\n",
    "**Key Features of Pandas**:Pandas offers an array of compelling features, including:\n",
    "\n",
    "- **DataFrame (core)**: A swift and efficient DataFrame object for seamless data manipulation, complete with built-in indexing.\n",
    "\n",
    "- **Data I/O**: Streamlined data reading and writing in various formats, such as Microsoft Excel, CSV, SQL databases, and more.\n",
    "\n",
    "- **Robust Data Manipulation**: Integrated and efficient methods for a wide spectrum of data manipulations, including handling missing data, subsetting, merging, and more.\n",
    "\n",
    "- **Temporary Data Handling**: Pandas excels in managing temporary data, making it the preferred choice for working with panel data (hence its name).\n",
    "\n",
    "- **Integration**: Smooth integration with other data analysis and machine learning libraries like scikit-learn, scipy, seaborn, and plotly.\n",
    "\n",
    "- **Widespread Usage**: Pandas enjoys widespread adoption across both private and academic sectors, making it a go-to tool for data enthusiasts.\n",
    "\n",
    "**Empowering Data Analysis**: Pandas provides high-level data structures and functions tailored to expedite your work with structured or tabular data. Since its debut in 2010, Pandas has played a pivotal role in elevating Python as a robust and efficient data analysis environment. \n",
    "\n",
    "The primary Pandas objects you'll encounter in this guide are the DataFrame—a column-oriented, tabular data structure with intuitive row and column labels—and the Series—a labeled, one-dimensional array. \n",
    "\n",
    "Pandas marries the high-performance principles of NumPy with the versatile data manipulation capabilities of spreadsheets and relational databases, such as SQL. It introduces sophisticated indexing features that simplify reshaping, slicing, aggregating, and selecting data subsets.\n",
    "\n",
    "In summary, Pandas empowers data analysts, scientists, and engineers to wield Python as a potent tool for a wide array of data-related tasks, revolutionizing the way structured data is managed and analyzed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-nutrition",
   "metadata": {},
   "source": [
    "![image](https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Source: [Forbes](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#1ba071616f63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6680d33",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0e351",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The first thing you should do will always be\n",
    "`pip install pandas`, `conda install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d4f4b",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Introduction to pandas data structures\n",
    "To get started with pandas, you'll need to get comfortable with its two working data structures: Series and DataFrame. Although they are not a universal solution to all problems, they provide a solid and easy-to-use foundation for most applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636439c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Series\n",
    "A Series is like a one-dimensional array, but with a twist. It holds a sequence of values, similar to what you'd find in NumPy arrays, but it also pairs each value with a label known as an index. This combination of data values and labels gives Series its power, making it a versatile tool for efficient data storage and retrieval.\n",
    "\n",
    "Here's how to create a basic Series using a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec68b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [10, 20, 30, 40, 50]\n",
    "series = pd.Series(data)\n",
    "print(series)\n",
    "\n",
    "# All methods implicit in serie\n",
    "print([i for i in dir(series) if \"_\" not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8006ecb",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The string representation of a Series displayed interactively shows the index on the left and the values ​​on the right. Since we didn't specify an index for the data, a default one consisting of the integers 0 to N - 1 (where N is the length of the data) is created. You can get the array representation and the index object of the Series through its values ​​and index attributes, respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f739f4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Another way to think of a Series is as a fixed-length ordered dict, since it is a mapping of index values ​​to data values. It can be used in many contexts where a dictionary could be used.\n",
    "If you have data contained in a Python dict, you can create a Series from it by passing the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type for pandas (class)\n",
    "type(series)\n",
    "\n",
    "# You can get the length too\n",
    "len(series)\n",
    "\n",
    "# Index of the series, can we use for loops here?\n",
    "series.index\n",
    "\n",
    "# The values on the series\n",
    "series.values\n",
    "\n",
    "# access by the index (as before)\n",
    "series[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ede87",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "When only one dict is passed, the resulting String index will have the keys of the dict in order. You can override this by passing the keys of the dict in the order you want them to appear in the resulting String:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a random dictionary\n",
    "some_data = {\n",
    "    \"Ohio\":4567,\n",
    "    \"Texas\": 5678,\n",
    "    \"Oregon\": 45678,\n",
    "    \"Utah\": 56789,\n",
    "    \"something else\": 43567\n",
    "}\n",
    "\n",
    "# Generate the type series\n",
    "my_series = pd.Series(some_data)\n",
    "my_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you think key and index are the same?\n",
    "list(some_data.keys()) == list(my_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value\n",
    "my_series.values\n",
    "\n",
    "# Get the index\n",
    "my_series.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a97ba9",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Handling Missing Data**: When constructing a Series with provided data and a custom index, Pandas will align the data based on the index labels. Any missing values in the data corresponding to the index will be marked as NaN (not a number). In Pandas, NaN represents missing or undefined values.\n",
    "\n",
    "For instance, let's consider creating a Series using predefined data and a custom index. In the following example, we have data for some states, but not all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000}\n",
    "states = ['Ohio', 'Texas', 'Oregon', 'Utah', 'California']\n",
    "\n",
    "# Create a Series with provided data and a custom index\n",
    "my_series = pd.Series(data=sdata, index=states)\n",
    "\n",
    "# The resulting Series contains the data for 'Ohio', 'Texas', and 'Oregon'\n",
    "# 'Utah' and 'California' are included in the index but have no data associated, hence they appear as NaN\n",
    "print(my_series)\n",
    "\n",
    "print('\\n')\n",
    "print(my_series.values)  # Displays the values of the Series\n",
    "print(my_series.index)   # Displays the index labels of the Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff1578",
   "metadata": {},
   "source": [
    "In this example, the Series `my_series` is created with data from `sdata` and a custom index `states`. The values corresponding to 'Ohio', 'Texas', and 'Oregon' are placed accordingly. However, 'Utah' and 'California' are present in the index but have no associated data, leading to NaN values in the Series.\n",
    "\n",
    "You can access the values and index of a Series using the `values` and `index` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb021a5",
   "metadata": {},
   "source": [
    "When working with NaN values, it's important to handle them appropriately in your data analysis, as operations on NaN may result in unexpected outcomes. You can use functions like `isna()` or `fillna()` to detect and manage missing values in your Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_series.isna())         # Returns a boolean Series indicating NaN values\n",
    "my_series.fillna(0, inplace=True)  # Fills NaN values with a specified value (e.g., 0) in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7674e",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686773c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    " In Pandas, a DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns). It is often compared to a spreadsheet or SQL table, as it provides a convenient way to store and manipulate data.\n",
    "\n",
    "**Understanding Dataframes**\n",
    "\n",
    "- **Series**: Before diving into DataFrames, it's essential to understand the concept of Series. A Series is a one-dimensional array-like object that can hold various data types. DataFrames are essentially collections of Series objects, each representing a column.\n",
    "\n",
    "- **Columns**: In a DataFrame, each Series represents a column. These columns can contain different types of data, such as integers, floats, or strings.\n",
    "\n",
    "- **Rows**: Rows in a DataFrame are organized by index labels. Each row corresponds to a specific entry, and you can access rows using their index labels.\n",
    "\n",
    "DataFrames are a powerful tool for data manipulation, analysis, and cleaning. They offer a structured way to work with data, making it easier to filter, sort, and compute statistics on datasets. We'll explore various DataFrame operations and functionalities in this guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca0560",
   "metadata": {},
   "source": [
    "#### From data types\n",
    "In Pandas, you can create DataFrames from various data sources, including dictionaries, lists, CSV files, SQL databases, and more. One common way to create a DataFrame is from a dictionary, where each key represents a column name, and the associated value is a list or array of data for that column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcad45",
   "metadata": {},
   "source": [
    "`from dictionaries with lists as values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0186c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with columns and data\n",
    "dict_states = {\n",
    "    \"state\": [\"Oregon\", \"Utah\", \"New Mexico\", \"Nebraska\"],\n",
    "    \"year\": [1900, 1898, 2000, 1900],\n",
    "    \"something_else\": [456, \"ssdsd\", 0.023, np.nan]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(dict_states)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162b19",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "In this example, we first import Pandas and NumPy libraries. Then, we create a dictionary `dict_states`, where each key represents a column name, and the associated value is a list of data for that column. We pass this dictionary to `pd.DataFrame()` to create a DataFrame named `df`. Finally, we display the DataFrame.\n",
    "\n",
    "When using Jupyter Notebook, Pandas DataFrame objects are displayed as more browser-friendly HTML tables, making it easier to view and explore the data interactively. You can find more options and customization details for DataFrame display in the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396870e",
   "metadata": {},
   "source": [
    "`from list of dictionaries`\n",
    "If you create a DataFrame from a list of dictionaries:\n",
    "\n",
    "- Each dictionary within the list represents a row in the DataFrame.\n",
    "- The keys within each dictionary become the column names of the DataFrame.\n",
    "- All dictionaries in the list must have the same structure in terms of keys to ensure consistency.\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Creating a DataFrame with consistent keys in each dictionary\n",
    "dict_states = {\n",
    "    \"state\": [\"Oregon\", \"Utah\", \"New Mexico\", \"Nebraska\"],\n",
    "    \"year\": [1900, 1898, 2000, 1900],\n",
    "    \"something_else\": [456, \"ssdsd\", 0.023, np.nan]\n",
    "}\n",
    "\n",
    "list_of_dictionaries = [\n",
    "    {\"state\":[\"Oregon\", \"Utah\", \"New Mexico\", \"Nebraska\"]}, # Each dictionary represents a row\n",
    "    {\"year\": [1900, 1898, 2000, 1900]}, # Corresponding values for the 'year' column\n",
    "    {\"something_else\": [456, \"ssdsd\", 0.023, np.nan]}, # Corresponding values for the 'something_else' column\n",
    "]\n",
    "\n",
    "# Creating a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(list_of_dictionaries)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df90706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Creating a DataFrame with varying dictionary structures\n",
    "list_of_dictionaries = [\n",
    "    {\"state\": \"Oregon\", \"year\": 1900, \"something_else\": \"oiuyghj\"}, # Each dictionary represents a row\n",
    "    {\"state\": \"Utah\", \"year\": 1989, \"something_else\": 678}, # Varying structures among dictionaries\n",
    "    {\"state\": \"New Mexico\", \"year\": 456, \"something_else\": 87, \"extra\": 98765} # Extra key 'extra' in one dictionary\n",
    "]\n",
    "\n",
    "# Creating a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(list_of_dictionaries)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c63fee",
   "metadata": {},
   "source": [
    "In the first example, we create a DataFrame `df` using a list of dictionaries `list_of_dictionaries`, where each dictionary represents a row with columns matching the keys. In the second example, we show that you can have variations in the structure of each dictionary as long as they have common keys.\n",
    "\n",
    "For more details, you can refer to the [pandas documentation on `from_dict`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbaae69",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### From path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818e830",
   "metadata": {},
   "source": [
    "`.csv`\n",
    "\n",
    "In this example, we load data into a DataFrame `df` using the `pd.read_csv()` function. Loading data from CSV files is a common operation when working with Pandas, as it allows you to bring external data into a DataFrame for analysis and manipulation. The resulting DataFrame, `df`, contains the structured data from the CSV file, making it accessible for further exploration and analysis.\n",
    "\n",
    "**Avocado Prices Dataset**: The \"Avocado Prices\" dataset, sourced from Kaggle, is a widely used dataset for data analysis and machine learning projects. It provides historical data on avocado prices and sales in various regions across the United States. This dataset is valuable for understanding trends in avocado pricing, sales volumes, and their relationship with different factors.\n",
    "\n",
    "**Key Attributes**:\n",
    "\n",
    "- **Columns**: The dataset includes several columns of information. Some of the key columns typically found in this dataset include:\n",
    "    - **Date**: The date of observation.\n",
    "    - **AveragePrice**: The average price of avocados.\n",
    "    - **Total Volume**: Total volume of avocados sold.\n",
    "    - **4046**: Volume of small Hass avocados sold.\n",
    "    - **4225**: Volume of large Hass avocados sold.\n",
    "    - **4770**: Volume of extra-large Hass avocados sold.\n",
    "    - **Total Bags**: Total bags of avocados sold.\n",
    "    - **Small Bags**: Bags of small avocados sold.\n",
    "    - **Large Bags**: Bags of large avocados sold.\n",
    "    - **XLarge Bags**: Bags of extra-large avocados sold.\n",
    "    - **Type**: The type of avocados, often categorized as conventional or organic.\n",
    "    - **Region**: The region or city within the United States where the data was recorded.\n",
    "\n",
    "- **Date Range**: The dataset covers a range of dates, enabling time-series analysis. You can examine how avocado prices and sales change over different seasons and years.\n",
    "\n",
    "- **Regions**: Information is provided for various regions or cities across the United States, allowing for the analysis of price and sales variations in different markets.\n",
    "\n",
    "- **Types**: The dataset distinguishes between different types of avocados, such as conventional and organic, which can be useful for comparing price trends between these categories.\n",
    "\n",
    "- **Volume**: Data on the total volume of avocados sold is available. This volume metric is often used to analyze market demand.\n",
    "\n",
    "- **Average Price**: The dataset contains the average price of avocados, a fundamental metric for understanding price trends.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- This dataset is commonly used for learning and practicing data analysis, data visualization, and regression modeling in data science and machine learning projects.\n",
    "\n",
    "- It serves as a valuable resource for understanding how to work with real-world data, draw insights, and make data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"datasets/avocado_kaggle.csv\")\n",
    "\n",
    "# Show dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985db2a6",
   "metadata": {},
   "source": [
    "`.xlsx`, `xls`, `xlsm`, `xlsb`, `odf`, `ods`, `odt`\n",
    "\n",
    "Pandas, a powerful data manipulation library in Python, provides robust functionality for reading and writing data to and from Excel files with various extensions. Excel files are commonly used to store structured data, making them a popular format for sharing and analyzing data in both business and research settings. (sometimes is too slow...)\n",
    "\n",
    "Pandas simplifies the process of handling Excel files, allowing you to seamlessly integrate data from spreadsheets into your data analysis workflows. Whether you're dealing with classic `.xls` files, modern `.xlsx` workbooks, or other Excel-compatible formats like `.xlsm`, `.xlsb`, `.odf`, `.ods`, and `.odt`, Pandas offers versatile tools to import and export data.\n",
    "\n",
    "**Key Features**:\n",
    "\n",
    "- **Read Excel Data**: Pandas provides functions to read Excel data into DataFrames, preserving the structure and formatting of worksheets. You can efficiently read data from multiple sheets within a workbook.\n",
    "\n",
    "- **Write Excel Data**: Pandas enables you to write DataFrames back to Excel files, allowing you to save your data along with any modifications or analyses you've performed.\n",
    "\n",
    "- **Compatibility**: Pandas supports various Excel file extensions, including `.xls`, `.xlsx`, `.xlsm`, `.xlsb`, `.odf`, `.ods`, and `.odt`, ensuring compatibility with different Excel versions and formats.\n",
    "\n",
    "- **Data Preservation**: When reading Excel files, Pandas retains data types, formulas, cell styles, and other attributes, ensuring data integrity.\n",
    "\n",
    "- **Data Manipulation**: Once data is loaded into Pandas DataFrames, you can use Pandas' extensive data manipulation and analysis capabilities to explore, clean, transform, and visualize your data.\n",
    "\n",
    "**Use Cases**:\n",
    "\n",
    "- Data Extraction: Extract structured data from Excel files to perform analysis, reporting, or visualization.\n",
    "\n",
    "- Data Integration: Combine data from multiple Excel sheets or workbooks into a single consolidated dataset.\n",
    "\n",
    "- Data Export: Save the results of data analysis conducted with Pandas back into Excel files for sharing or further processing.\n",
    "\n",
    "- Automation: Automate data extraction and manipulation tasks by incorporating Pandas into your data pipeline or workflow.\n",
    "\n",
    "Pandas makes working with Excel files a breeze, allowing you to harness the power of Python for your data analysis projects while seamlessly interacting with Excel data.\n",
    "\n",
    "To get started, explore Pandas' extensive documentation on [Excel File I/O](https://pandas.pydata.org/docs/reference/io.html#excel) to learn about the various methods and options available for reading and writing Excel files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "# Another dataset\n",
    "df_from_excel = pd.read_excel(\"datasets/Online Retail.xlsx\", engine=\"openpyxl\", nrows=5)\n",
    "df_from_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9178b7",
   "metadata": {},
   "source": [
    "`Reading different sheeets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Excel file\n",
    "excel_file_path = \"datasets/Online Retail.xlsx\"\n",
    "\n",
    "# Reading the Default Tab (First One)\n",
    "# Use the read_excel function to read the first sheet of the Excel file (default behavior)\n",
    "df_default_tab = pd.read_excel(excel_file_path, engine=\"openpyxl\", nrows=5)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "df_default_tab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Another Tab (e.g., \"new_tab\")\n",
    "# Specify the sheet name as a string to read a specific sheet from the Excel file\n",
    "df_new_tab = pd.read_excel(excel_file_path, engine=\"openpyxl\", sheet_name=\"new_tab\", nrows=5)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame from the \"new_tab\" sheet\n",
    "df_new_tab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9ea58",
   "metadata": {},
   "source": [
    "#### From databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcaa3c",
   "metadata": {},
   "source": [
    "`sql`: [docs](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2380f09",
   "metadata": {},
   "source": [
    "```python\n",
    "from sqlite3 import connect\n",
    "\n",
    "conn = connect(':memory:')\n",
    "df = pd.read_sql('SELECT column_1, column_2 FROM sample_data', conn)\n",
    "\n",
    "df.to_sql('test_data', conn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92c595",
   "metadata": {},
   "source": [
    "`mongodb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cee4b",
   "metadata": {},
   "source": [
    "```python\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.database_name\n",
    "collection = db.collection_name\n",
    "data = pd.DataFrame(list(collection.find()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7fbf8",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Exploratory analysis of a dataframe\n",
    "\n",
    "In this section, we'll dive into the world of exploratory data analysis (EDA) using Pandas. EDA is a crucial step in the data analysis process, where we get to know our data, understand its characteristics, and uncover initial insights. We'll use a DataFrame, `df`, loaded from the \"Advertising.csv\" dataset as an example to perform various exploratory analyses. Let's begin by loading the dataset and getting a glimpse of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"datasets/Advertising.csv\")\n",
    "\n",
    "# Check first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9482a5",
   "metadata": {},
   "source": [
    "### Metadata and information\n",
    "\n",
    "`shape, columns, dtypes, info, describe`\n",
    "\n",
    "When working with data in a DataFrame, it's crucial to understand the basic characteristics and structure of the dataset. To gain insights into the data, we can retrieve meta information about the DataFrame. This information includes details such as the shape of the DataFrame, column names, data types, general information, and a statistical summary of the data.\n",
    "\n",
    "In this section, we'll explore how to use various Pandas functions to obtain essential meta information about your DataFrame. This knowledge will help you better understand and prepare your data for analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21068edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the DataFrame (rows, columns)\n",
    "data_shape = df.shape\n",
    "print(f\"\\nShape of the DataFrame: {data_shape}\")\n",
    "\n",
    "# Column names\n",
    "column_names = df.columns\n",
    "print(f\"\\nColumn Names: {column_names}\")\n",
    "\n",
    "# Data types of each column\n",
    "data_types = df.dtypes\n",
    "print(f\"\\nData Types:\\n{data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c14a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Information about the DataFrame\n",
    "data_info = df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "data_summary = df.describe()\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7886577",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Previsualization\n",
    "Before diving into in-depth analysis and manipulation of your dataset, it's often helpful to get a quick glimpse of what the data looks like. Pandas provides two useful methods, `head()` and `tail()`, to help you previsualize the beginning and end of your DataFrame.\n",
    "\n",
    "In this section, we'll explore how to use these methods to display a subset of your data, making it easier to understand the structure and content of your DataFrame. These simple yet powerful tools are the first step in getting acquainted with your data, allowing you to identify any immediate patterns or issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209b64c",
   "metadata": {},
   "source": [
    "`head` and `tail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6520b",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "By default head shows me the first 5 rows, I can see some more or less by passing a number as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71547174",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Sort and organize a dataframe\n",
    "Organizing and sorting your data is a fundamental part of data analysis. In this section, we'll explore how to order a DataFrame using the Pandas library. By sorting data, you can gain valuable insights, identify trends, and make your data more accessible for analysis.\n",
    "\n",
    "We'll cover various scenarios, such as sorting by one or more columns, in ascending or descending order, and selecting specific columns to view. Understanding how to arrange your data effectively can significantly enhance your ability to extract meaningful information from it.\n",
    "\n",
    "Let's dive into the different ways to order and arrange your data using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from a CSV file\n",
    "df = pd.read_csv(\"datasets/avocado_kaggle.csv\")\n",
    "\n",
    "# Sorting by a single column in descending order (most recent year first)\n",
    "df.sort_values(by=\"year\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by multiple columns in descending order (year and region)\n",
    "df.sort_values(by=[\"year\", \"region\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by a single column in descending order (region)\n",
    "df.sort_values(by=\"region\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by a single column in descending order (year)\n",
    "df.sort_values(by=\"year\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128be49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specific columns (year and region) after sorting\n",
    "df.sort_values(by=\"year\", ascending=False)[[\"year\", \"region\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access the df's columns\n",
    "print(df.columns)\n",
    "# Creating a list of column names to create a subset of columns\n",
    "subset_columns = list(df.columns)[1:4]\n",
    "\n",
    "# Selecting a subset of columns using the list of column names\n",
    "df[subset_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd2f0d",
   "metadata": {},
   "source": [
    "`sample`\n",
    "\n",
    "In data analysis, it's often essential to work with a representative subset of your dataset for various purposes, such as data exploration, testing, or model training. Pandas provides the `sample` method to facilitate random sampling of rows from a DataFrame. This method allows you to obtain random rows or a specific fraction of your data, making it a valuable tool for statistical analysis and machine learning tasks. In this section, we'll explore how to use the `sample` method to extract random samples from a DataFrame and understand its various options and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afaa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling a random single row from the DataFrame\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling a random fraction (20%) of rows from the DataFrame\n",
    "df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4bc9c",
   "metadata": {},
   "source": [
    "`display`\n",
    "When working with Jupyter Notebook or JupyterLab, you can use the `display` function to render Pandas DataFrames in a more visually appealing and interactive format. While Pandas' default tabular display is informative, the `display` function provides additional flexibility and customization options.\n",
    "\n",
    "By using `display`, you can take advantage of the enhanced table formatting capabilities of Jupyter environments, including sortable columns, responsive design, and improved visual representation of your data. It's especially useful when dealing with larger datasets or when you want to present your data in a cleaner and more interactive way for data exploration or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ae834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame using the display function (equivalent to print)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7446f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### NaN values\n",
    "\n",
    "In data analysis, missing data is a common occurrence and can significantly impact the accuracy and reliability of your analyses. One way to represent missing data in Pandas and many other data analysis libraries is by using the special value \"NaN,\" which stands for \"Not A Number.\"\n",
    "\n",
    "NaN is essentially a placeholder for missing or undefined data points, and it is typically treated as a floating-point value. This allows Pandas to work with missing data while maintaining data types within a DataFrame.\n",
    "\n",
    "Handling NaN values is a crucial aspect of data cleaning and preprocessing, as it can affect statistical calculations, visualizations, and machine learning models. In this section, we'll explore various techniques and functions in Pandas for dealing with missing data and ensuring that your data analysis yields accurate and meaningful results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset info\n",
    "df.info()\n",
    "\n",
    "pd.isnull(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "missing_values = pd.isnull(df)\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = missing_values.sum()\n",
    "\n",
    "# Count columns with missing values\n",
    "columns_with_missing = missing_counts[missing_counts > 0].count()\n",
    "\n",
    "# Check if all columns have missing values\n",
    "all_columns_missing = missing_counts.all()\n",
    "\n",
    "# Calculate the total number of missing values\n",
    "total_missing_values = missing_counts.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_counts)\n",
    "print(\"\\nNumber of Columns with Missing Values:\", columns_with_missing)\n",
    "print(\"All Columns Have Missing Values:\", all_columns_missing)\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\", total_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35a4ce",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe626929",
   "metadata": {},
   "source": [
    "### Index & Columns\n",
    " \n",
    "What does an index mean?\n",
    "\n",
    "What properties must they obey?\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca94a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lets take a step back and see what they are made of\n",
    "#### DATAFRAMES ####\n",
    "\n",
    "# Define a df\n",
    "myarray = np.random.random((10,5))\n",
    "print(myarray)\n",
    "\n",
    "a = pd.DataFrame(myarray)\n",
    "#a = pd.DataFrame([['joao',2,3],['Melissa',8,7],['CR7',9,5]])\n",
    "\n",
    "# Define it's column names\n",
    "a.columns = [\"first\",\"second\",\"third\",\"fourth\",\"fifth\"]\n",
    "\n",
    "# Same with rows\n",
    "print(list(a.index))\n",
    "a.head()\n",
    "\n",
    "\n",
    "#### SERIES ####\n",
    "# Dataframes are actually a 2D Array of SERIES\n",
    "a = np.random.random(3)\n",
    "myseries = pd.Series(a)\n",
    "\n",
    "# An index in a pandas dataframe/dataseries a unique key that completely identifies a row (in a distinct way)\n",
    "myseries\n",
    "\n",
    "# Two fundamental objects in pandas\n",
    "\n",
    "#series\n",
    "a = pd.Series(np.random.random(20))\n",
    "\n",
    "#dataframes\n",
    "a = pd.DataFrame(np.random.random((3,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2319c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "print(a.columns)\n",
    "print(list(a.columns))\n",
    "\n",
    "#what is i want to change all column names?\n",
    "a.columns = ['fourth','second','third','first','fifth']\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1daa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change some columns names\n",
    "a = a.rename(columns = {'first': 'FIRST'})\n",
    "a\n",
    "a = a.rename(columns = {'FIRST': 'first'})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8920fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re Order the columns\n",
    "#a.columns = ['first','second','third','fourth','fifth']\n",
    "\n",
    "#or\n",
    "\n",
    "column_order = ['first','second','third','fourth','fifth']\n",
    "a = a[column_order]\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX\n",
    "# Explain what index is\n",
    "\n",
    "a.index\n",
    "list(a.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53df054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX: imutable -> change all or none\n",
    "a.index = ['first-row', 'second-row', 'third-row']\n",
    "#a.index[1] = '2nd line'\n",
    "\n",
    "# See how index is imutable -> Be carefull here: you can change the indexes, if you change it all at once\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bff5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX: typical assignements\n",
    "#what types of indices are there?\n",
    "\n",
    "a.index = ['melissa', 'joao', 'cr7']\n",
    "# By order of importance in the class\n",
    "a\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "#new_index = pd.date_range(end = dt.date.today(), periods = 10,)\n",
    "new_index = pd.date_range(end = dt.date.today(), periods = 3,)\n",
    "new_index\n",
    "\n",
    "a.index = new_index\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea30a69",
   "metadata": {},
   "source": [
    "### LOC & ILOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f688e",
   "metadata": {},
   "source": [
    "![iloc](https://miro.medium.com/v2/resize:fit:720/format:webp/1*dYtynwab99wnMqfgyPUd3w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6740fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOC & ILOC ####\n",
    "# Slicing dataframes will be something very frequent\n",
    "\n",
    "## INDEXING\n",
    "#indexing dataframes\n",
    "a['first'][0:2]\n",
    "\n",
    "#indexing dataframes\n",
    "print(a[:][1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets increase the size of a\n",
    "a = pd.DataFrame(np.random.random((100,5)))\n",
    "a.columns = [\"first\",\"second\",\"third\",\"fourth\",\"fifth\"]\n",
    "\n",
    "# Lets not use List Comprehension, as the legilibility is also important\n",
    "index_list = []\n",
    "for i in range(100):\n",
    "  index_list.append('row '+str(i))\n",
    "\n",
    "a.index = index_list\n",
    "a.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS VERY IMPORTANT\n",
    "\n",
    "## LOC\n",
    "# or using the locate method\n",
    "\n",
    "a.loc['row 0'] # -> This returns a series\n",
    "#a.loc['row 0','row 1'] # What does this return? an error\n",
    "a.loc[['row 0','row 1']]\n",
    "\n",
    "## ILOC\n",
    "# or using the index-locate method\n",
    "# Same as Lists: from a[x:y], will return from a[x] including, to a[y] excluding\n",
    "\n",
    "#a.iloc[0:2]\n",
    "a.iloc[[x for x in range (50) if x%2==0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7e8af",
   "metadata": {},
   "source": [
    "### Filter by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILTER ####\n",
    "# Filter by Categorical data\n",
    "\n",
    "### OVERVIEW ON DATAFRAMES ###\n",
    "# Dont copy for now - I will repeat all this\n",
    "# pip install xlrd\n",
    "import pandas as pd\n",
    "\n",
    "orders = pd.read_excel(\"datasets/Sample - Superstore.xls\")\n",
    "display(orders.head())\n",
    "\n",
    "condition = orders['Category'] == \"Office Supplies\"\n",
    "condition\n",
    "#orders[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by numerical data (intervals)\n",
    "# order [    condition         ]\n",
    "#orders[    orders['Sales'] == 22638.48     ]\n",
    "orders[  orders['Sales']  > 5000  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39455a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's review what happens when we filter by row\n",
    "\n",
    "# We create a boolean vector with the same size as the rows in the data set\n",
    "condition = orders['Sales'] == 9449.950\t\n",
    "print(condition)\n",
    "display(orders[condition])\n",
    "\n",
    "print(orders['Sales'].max())\n",
    "#orders[orders['Sales'] == orders['Sales'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The condition might be an interval of things (not a fixed value)\n",
    "condition = (orders['Sales'] > 5000) & (orders['Sales'] < 10000)\n",
    "#condition = orders['Sales'] > orders['Sales'].max()*0.5\n",
    "orders[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out the top 5 orders\n",
    "\n",
    "orders = orders.sort_values(by=['Sales'], ascending=False)\n",
    "#orders.head()\n",
    "\n",
    "top_five = orders.iloc[0:5]\n",
    "top_five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bc6d3",
   "metadata": {},
   "source": [
    "## Business Challenge: Analyzing Avocado Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ceff9",
   "metadata": {},
   "source": [
    "Business Challenge: Analyzing Avocado Sales\n",
    "\n",
    "In this exercise, we'll delve into a real-world business scenario involving avocado sales data. Avocado consumption has surged in recent years, and you've been hired by a regional grocery store chain to gain insights from their sales data. The store wants to understand the trends, pricing strategies, and factors influencing avocado sales to make informed decisions and improve profitability.\n",
    "\n",
    "### Introduction to the Avocado Dataset\n",
    "\n",
    "The dataset you'll be working with contains information about avocado sales across different regions in the United States. The data includes details like date, average price, total volume sold, region, and more.\n",
    "\n",
    "**Your Mission:** Using Pandas, perform a comprehensive analysis to answer critical business questions. Here are some of the tasks you'll need to accomplish:\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Data Loading:** Begin by loading the Avocado dataset (`avocado.csv`) into a Pandas DataFrame.\n",
    "\n",
    "2. **Data Exploration:** Conduct an exploratory data analysis to understand the dataset's structure, including the number of rows and columns, data types, and any missing values.\n",
    "\n",
    "3. **Time-Series Analysis:** Analyze the avocado sales over time. Identify seasonal trends, and determine if there are any patterns related to pricing and volume.\n",
    "\n",
    "4. **Regional Analysis:** Investigate avocado sales by region. Which regions are the top performers in terms of sales volume and pricing? Are there any regions that require specific attention?\n",
    "\n",
    "5. **Price and Volume Trends:** Determine how changes in avocado prices affect sales volume. Are there price points that drive higher or lower sales? \n",
    "\n",
    "6. **Price Elasticity:** Calculate the price elasticity of demand for avocados. This will help the store understand how sensitive sales are to price changes.\n",
    "\n",
    "7. **Recommendations:** Based on your analysis, provide actionable recommendations to the grocery store chain. What pricing strategies should they consider? Are there specific regions where they can improve sales?\n",
    "\n",
    "### Getting Started:\n",
    "\n",
    "To get started, load the Avocado dataset and begin your data exploration. Utilize Pandas for data cleaning, visualization, and analysis. As you progress through the tasks, document your findings and insights to present to the grocery store chain's management.\n",
    "\n",
    "Remember, Pandas is a powerful tool that can help businesses make data-driven decisions. This exercise will give you hands-on experience in data analysis and showcase the valuable insights that can be extracted from real-world data.\n",
    "\n",
    "Now, let's dive into the world of avocado sales and start making data-driven recommendations to boost profitability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30faaf9e",
   "metadata": {},
   "source": [
    "## Recap: Key Points About Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f352ca",
   "metadata": {},
   "source": [
    "RECAP\n",
    "\n",
    "- **Pandas is a Powerful Library:** Pandas is a versatile Python library used for working with structured data efficiently.\n",
    "\n",
    "- **Built on NumPy:** It's built on top of NumPy, which makes it incredibly fast and efficient for data manipulation.\n",
    "\n",
    "- **Widely Used:** Pandas is widely adopted by other data libraries and tools, making it a fundamental part of the data ecosystem.\n",
    "\n",
    "- **Tabular Data:** Pandas excels at handling tabular data, which consists of rows and columns.\n",
    "\n",
    "- **Python and Pandas Together:** When working with data, you often use both Python and Pandas in tandem to achieve your goals.\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "- **Data Preview:** You can quickly preview your data using methods like `head()`, `tail()`, and `sample()` to inspect the beginning, end, or random portions of your dataset.\n",
    "\n",
    "- **Data Overview:** The `info()` and `describe()` methods provide an overview of your data, including data types and descriptive statistics.\n",
    "\n",
    "- **Unique Values:** Use `unique()` or `value_counts()` to find unique values or count occurrences in a column.\n",
    "\n",
    "- **Sorting:** You can sort your data using the `sort_values()` method, which is helpful for arranging data by specific columns.\n",
    "\n",
    "- **Subsetting Data:** Subsetting allows you to select specific rows or columns. Use square brackets (`[]`) to extract columns or rows based on conditions.\n",
    "\n",
    "### Creating DataFrames\n",
    "\n",
    "- **From Lists of Dictionaries:** You can create DataFrames from lists of dictionaries, where each dictionary represents a row.\n",
    "\n",
    "- **From Dictionaries with Lists:** Alternatively, DataFrames can be created from dictionaries with lists as values. Be mindful of ensuring lists have the same length.\n",
    "\n",
    "- **Reading Data:** Pandas provides functions like `read_csv()` for reading data from various sources, such as CSV files, URLs, SQL databases, Excel files, and more. You can use both absolute and relative paths, and even load data from remote sources like GitHub repositories.\n",
    "\n",
    "These are some of the essential concepts and operations in Pandas, allowing you to manipulate and explore your data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f6df9",
   "metadata": {},
   "source": [
    "## Pandas usual methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f340806",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "```python\n",
    "df.head() # prints the head, default 5 rows\n",
    "df.tail() # set the tail, default 5 rows\n",
    "df.describe() # statistical description\n",
    "df.info() # df information\n",
    "df.columns # show column\n",
    "df.index # show index\n",
    "df.dtypes # show column data types\n",
    "df.plot() # make a plot\n",
    "df.hist() # make a histogram\n",
    "df.col.value_counts() # counts the unique values ​​of a column\n",
    "df.col.unique() # returns unique values ​​from a column\n",
    "df.copy() # copies the df\n",
    "df.drop() # remove columns or rows (axis=0,1)\n",
    "df.dropna() # remove nulls\n",
    "df.fillna() # fills nulls\n",
    "df.shape # dimensions of the df\n",
    "df._get_numeric_data() # select numeric columns\n",
    "df.rename() # rename columns\n",
    "df.str.replace() # replace columns of strings\n",
    "df.astype(dtype='float32') # change the data type\n",
    "df.iloc[] # locate by index\n",
    "df.loc[] # locate by element\n",
    "df.transpose() # transposes the df\n",
    "df.T\n",
    "df.sample(n, frac) # sample from df\n",
    "df.col.sum() # sum of a column\n",
    "df.col.max() # maximum of a column\n",
    "df.col.min() # minimum of one column\n",
    "df[col] # select column\n",
    "df.col\n",
    "df.isnull() # null values\n",
    "df.isna()\n",
    "df.notna() # not null values\n",
    "df.drop_duplicates() # remove duplicates\n",
    "df.reset_index(inplace=True) # reset the index and overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-jefferson",
   "metadata": {},
   "source": [
    "## Further materials\n",
    "\n",
    "* [Read the docs!](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "* [Cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "* [Exercises to practice](https://github.com/guipsamora/pandas_exercises)\n",
    "* [More on merge, concat, and join](https://realpython.com/pandas-merge-join-and-concat/#pandas-join-combining-data-on-a-column-or-index). And [even more!](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034bb574",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Avocado dataset into a Pandas DataFrame\n",
    "df = pd.read_csv(\"datasets/avocado_kaggle.csv\")\n",
    "\n",
    "# Data Exploration\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Time-Series Analysis\n",
    "# Convert the 'Date' column to a datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Group data by date to analyze trends\n",
    "monthly_sales = df.groupby(df['Date'].dt.to_period(\"M\"))['Total Volume'].sum()\n",
    "monthly_sales.plot(figsize=(12, 6))\n",
    "plt.title('Monthly Avocado Sales Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Volume Sold')\n",
    "plt.show()\n",
    "\n",
    "# Regional Analysis\n",
    "# Identify the top-performing regions in terms of sales volume and pricing\n",
    "top_regions_volume = df.groupby('region')['Total Volume'].sum().nlargest(5)\n",
    "top_regions_price = df.groupby('region')['AveragePrice'].mean().nlargest(5)\n",
    "\n",
    "print(\"Top 5 Regions by Sales Volume:\")\n",
    "print(top_regions_volume)\n",
    "print(\"\\nTop 5 Regions by Average Price:\")\n",
    "print(top_regions_price)\n",
    "\n",
    "# Price and Volume Trends\n",
    "# Analyze how changes in avocado prices affect sales volume\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df['AveragePrice'], df['Total Volume'], alpha=0.5)\n",
    "plt.title('Price vs. Volume')\n",
    "plt.xlabel('Average Price')\n",
    "plt.ylabel('Total Volume Sold')\n",
    "plt.show()\n",
    "\n",
    "# Price Elasticity\n",
    "# Calculate the price elasticity of demand\n",
    "df['Price Elasticity'] = df['Total Volume'] / df['AveragePrice']\n",
    "df['Price Elasticity'].describe()\n",
    "\n",
    "# Recommendations\n",
    "# Provide recommendations based on analysis\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Focus on regions with high sales volume, such as\", top_regions_volume.index[0])\n",
    "print(\"2. Monitor price elasticity closely and consider adjusting prices strategically.\")\n",
    "print(\"3. Analyze seasonal trends for potential marketing campaigns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.766px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
